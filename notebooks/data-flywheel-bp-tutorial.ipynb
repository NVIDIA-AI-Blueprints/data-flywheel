{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Discover Cost-Efficient AI Customer Service Agents with NVIDIA Data Flywheel Blueprint\n",
    "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W)\n",
    "\n",
    "This notebook demonstrates the end-to-end process of using the Data Flywheel Blueprint to continuously identify and promote more cost-efficient agents for an AI-powered customer service assistant.\n",
    "\n",
    "### [Data Flywheel Blueprint](https://build.nvidia.com/nvidia/build-an-enterprise-data-flywheel)\n",
    "\n",
    "The NVIDIA Data Flywheel Blueprint provides a systematic, automated solution to refine and redeploy optimized models that maintain accuracy targets while lowering resource demands. This blueprint establishes a self-reinforcing data flywheel, using production traffic logs and institutional knowledge to continuously improve model efficiency and accuracy.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/update-launchable/docs/images/data-flywheel-blueprint.png\" alt=\"AIVA\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### [AI Virtual Assistant for Customer Service](https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service/blueprintcard)\n",
    "\n",
    "The AI virtual assistant (AIVA) for customer service NIM Agent Blueprint, powered by NVIDIA NeMo Retriever™ and NVIDIA NIM™ microservices, along with retrieval-augmented generation (RAG), offers a streamlined solution for enhancing customer support. It enables context-aware, multi-turn conversations, providing general and personalized Q&A responses based on structured and unstructured data, such as order history and product details. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.githubusercontent.com/media/NVIDIA-AI-Blueprints/ai-virtual-assistant/refs/heads/main/docs/imgs/IVA-blueprint-diagram-r5.png\" alt=\"AIVA\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "AIVA employs a multi-agent architecture, where each user query is processed through multiple specialized agents, requiring several LLM calls per interaction. While this design ensures accuracy, it also introduces high latency, especially when using large models like Llama-3.3-70B or Llama-3.1-Nemotron-Ultra-253B-v1, potentially impacting user experience.\n",
    "\n",
    "To mitigate this, we leverage the Data Flywheel Blueprint to distill smaller models that maintain comparable accuracy to larger ones. By utilizing production traffic from AIVA, this approach significantly reduces response latency while preserving model quality and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Data Flywheel Blueprint Setup](#1)\n",
    "2. [AI Virtual Assistant Application Setup](#2)\n",
    "3. [Run a Flywheel Job](#3)\n",
    "4. [Monitor Job Status](#4)\n",
    "5. [Optional: Show Continuous Improvement](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## 1. Data Flywheel Blueprint Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set NGC API key following the instructions at [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['NGC_API_KEY'] = getpass(\"Enter your NGC API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Clone the data flywheel repo and fetch data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step presents two options:\n",
    "* **Step 2 (Option 1) NVIDIA Brev Launchable Setup:** The instructions below apply **only** to users running this notebook via the Brev Launchable.\n",
    "  \n",
    "NVIDIA Brev is a developer-friendly platform that makes it easy to run, train, and deploy ML models on cloud GPUs without the hassle of setup—it comes preloaded with Python, CUDA, and Docker so you can get started fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/NVIDIA-AI-Blueprints/data-flywheel.git\n",
    "cd data-flywheel\n",
    "sudo apt-get update && sudo apt-get install -y git-lfs\n",
    "git lfs install\n",
    "git-lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir / \"data-flywheel\"\n",
    "data_dir = project_root / \"data\"\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory changed to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step 2 (Option 2) Self-Hosted Notebook Setup:** The instructions below apply **only** to users running this notebook on their own setup (i.e., if you followed the pre-requisites in the [Data-Flywheel Blueprint Github README](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/tree/main/notebooks#prerequisites) for hardware and software requirements, to clone the repo, and start Jupyter Notebook.)\n",
    "\n",
    "    > **Note:** If you are using a [Brev Launchable](https://brev.nvidia.com/launchable/deploy/now?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W), please follow **Option 1** above in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important: Uncomment and run this cell in a self-hosted notebook setup\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# notebook_dir = Path.cwd()\n",
    "# project_root = notebook_dir.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Install python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "source .venv/bin/activate\n",
    "python -m ensurepip --upgrade\n",
    "python -m pip install --upgrade pip setuptools wheel\n",
    "pip install elasticsearch==8.17.2 pandas>=2.2.3 matplotlib==3.10.3 pydantic==2.11.3 pydantic-settings==2.9.1 nemo-microservices==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Update `config/config.yaml` to use remote LLM as judge. By default, the Data Flywheel Blueprint deploys `LLama-3.3-70B-instruct` locally for LLM as a judge, which requires 4 GPUs. But for the launchable, we will choose the remote LLM judge and use the `LLama-3.1-70B-instruct` NIM hosted on [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct). Feel free to skip this setp if you plan to use a local LLM as the judge.\n",
    "\n",
    "For this notebook, we will use only `meta/llama-3.2-1b-instruct`, `meta/llama-3.2-3b-instruct`, and `nvidia/llama-3.1-nemotron-nano-8b-v1` in the flywheel but you can uncomment other models in the yaml file to include in the flywheel run. You can also change other config settings such as data split and training hyperparameters as desired. Users can also disable model customization (fine-tuning) by setting `customization_enabled` to false.\n",
    "\n",
    "For more details on the configuration for running data flywheel, please refer to [Configuration Guide](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/blob/main/docs/03-configuration.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "\n",
    "new_llm_block = dedent(\"\"\"\\\n",
    "llm_judge_config:\n",
    "  deployment_type: \"remote\"\n",
    "  url: \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "  model_name: \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "new_nims_block = dedent(\"\"\"\\\n",
    "nims:      \n",
    "  - model_name: \"meta/llama-3.2-1b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.2-1b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "  - model_name: \"meta/llama-3.2-3b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.2-3b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "  - model_name: \"meta/llama-3.1-8b-instruct\"\n",
    "    model_type: \"llm\"\n",
    "    context_length: 8192\n",
    "    gpus: 1\n",
    "    pvc_size: 25Gi\n",
    "    tag: \"1.8.3\"\n",
    "    customization_enabled: true\n",
    "    customizer_configs:\n",
    "      target: \"meta/llama-3.1-8b-instruct@2.0\"\n",
    "      gpus: 1\n",
    "      max_seq_length: 8192\n",
    "\n",
    "  # - model_name: \"nvidia/llama-3.1-nemotron-nano-8b-v1\"\n",
    "  #   model_type: \"llm\"\n",
    "  #   context_length: 4096\n",
    "  #   gpus: 1\n",
    "  #   pvc_size: 25Gi\n",
    "  #   tag: \"1.8.4\"\n",
    "  #   customization_enabled: true\n",
    "  #   customizer_configs:\n",
    "  #     target: \"nvidia/nemotron-nano-llama-3.1-8b@1.0\"\n",
    "  #     gpus: 1\n",
    "  #     max_seq_length: 4096\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "text = config_path.read_text()\n",
    "\n",
    "def replace_block(yaml_text: str, key: str, new_block: str) -> str:\n",
    "    pattern = rf\"(?ms)^({re.escape(key)}:[\\s\\S]*?)(?=^\\S|\\Z)\"\n",
    "    return re.sub(pattern, new_block, yaml_text)\n",
    "\n",
    "text = replace_block(text, \"llm_judge_config\", new_llm_block)\n",
    "text = replace_block(text, \"nims\", new_nims_block)\n",
    "\n",
    "config_path.write_text(text)\n",
    "print(\"config.yaml updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use remote LLM as judge, we will set the API key to access the remote LLM. You can create an API Key at https://build.nvidia.com/settings/api-keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NVIDIA_API_KEY'] = getpass(\"Enter your NVIDIA API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Start data flywheel service, which involves first deploying the Nemo Microservices and then bring up the data flywheel service via docker compose with MLFlow enabled. This step may take about 10 minutes.\n",
    "\n",
    "> **Note:** The `deploy-nmp.sh` script automates the deployment of NeMo Microservices. For manual setup or advanced configuration, please consult the [NeMo Microservices documentation](https://docs.nvidia.com/nemo/microservices/latest/get-started/platform-prereq.html#beginner-tutorial-prerequisites).\n",
    "\n",
    "If you choose to manually deploy the Nemo Microservices Platform, then make sure you update the `nmp_config` field in the `config/config.yaml` with the correct base urls. The default is:\n",
    "```\n",
    "nmp_config:\n",
    "      nemo_base_url: \"http://nemo.test\"\n",
    "      nim_base_url: \"http://nim.test\"\n",
    "      datastore_base_url: \"http://data-store.test\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "log() {\n",
    "  echo -e \"\\033[1;32m[INFO]\\033[0m $1\"\n",
    "}\n",
    "\n",
    "echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "chmod +x scripts/deploy-nmp.sh\n",
    "./scripts/deploy-nmp.sh --progress\n",
    "log \"Starting data flywheel service...\"\n",
    "export COMPOSE_PROFILES=mlflow && docker compose -f deploy/docker-compose.yaml up -d --build >> flywheel_deploy.log 2>&1\n",
    "log \"Data flywheel service started successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2\"></a>\n",
    "## 2. AI Virtual Assistant Application Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we’ll use AIVA as the reference application to illustrate how the Data Flywheel Blueprint can be leveraged to reduce inference latency.\n",
    "We start by deploying the AIVA application. To enable observability, the original AIVA implementation is wrapped with the NeMo Agent Toolkit, which provides a Data Flywheel plugin for seamlessly exporting runtime traces in the schema expected by the [Data Flywheel Blueprint](https://docs.nvidia.com/nemo/agent-toolkit/latest/workflows/observe/observe-workflow-with-data-flywheel.html) to its Elasticsearch instance.\n",
    "\n",
    "For additional details on wrapping AIVA or other GenAI applications with the NeMo Agent Toolkit, see the [NAT migration guide](./NAT_MIGRATION_GUIDE.md).\n",
    "\n",
    "### 2.1 Deploy AIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repository\n",
    "cd ..\n",
    "git clone --branch nat-dfw-integration https://github.com/NVIDIA-AI-Blueprints/ai-virtual-assistant.git\n",
    "cd ai-virtual-assistant\n",
    "\n",
    "export APP_CHAT_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export APP_CHAT_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export PRIMARY_ASSISTANT_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export PRIMARY_ASSISTANT_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export ORDER_STATUS_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export ORDER_STATUS_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export RETURN_PROCESSING_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export RETURN_PROCESSING_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export APP_LLM_MODELENGINE=nvidia-ai-endpoints\n",
    "export DATA_FLYWHEEL_CLIENT_ID=nat-ai-virtual-assistant\n",
    "export DATA_FLYWHEEL_ENDPOINT=http://localhost:9200\n",
    "export DATA_FLYWHEEL_ES_INDEX=flywheel\n",
    "\n",
    "docker compose -f deploy/compose/docker-compose.nat.yaml up -d --build >> aiva_deploy.log 2>&1\n",
    "docker compose -f src/ingest_service/docker-compose.yaml run --rm ingest-client >> aiva_deploy.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: deploying the full AIVA will application take roughly 10 to 20 mins.\n",
    "\n",
    "### 2.2 Interact with AIVA and View Logs\n",
    "\n",
    "Next navigate to your Brev instance page, go to the **Access** tab, select **Using Secure Links**, find and click the link that looks like `https://aiva-xxxxxx.brevlab.com` which will take you the AIVA application UI(shown below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aiva_ui](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/fce0d3864d3587f1545fea8fb2f451ae02c082cd/notebooks/img/aiva-ui.png)\n",
    "\n",
    "Below you will find the call graph for AIVA.\n",
    "\n",
    "![AIVA_call_graph](https://media.githubusercontent.com/media/NVIDIA-AI-Blueprints/ai-virtual-assistant/refs/heads/aiq-dfw-integration/src/aiq_agent/aiva_agent/graph_image.png)\n",
    "\n",
    "You can see that AIVA is a multi-agent application implemented with LangGraph, where each agent node (e.g., primary_assistant, order_status, return_processing, etc.) is wrapped with the NeMo Agent Toolkit’s `@register_function` decorator to enable observability and workload scoping. To learn how to migrate the original AIVA implementation to the NeMo Agent Toolkit and gain built-in observability and Data Flywheel capabilities, check out this [guide](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/blob/fce0d3864d3587f1545fea8fb2f451ae02c082cd/notebooks/NAT_MIGRATION_GUIDE.md).\n",
    "\n",
    "The Data Flywheel Blueprint uses workload identifiers (`workload_id`) to organize traces for targeted model optimization. When AIVA is wrapped with the NeMo Agent Toolkit, each agent node functions as a registered component and automatically receives a workload_id corresponding to its function/node name. For example, LLM calls within the order_status agent are associated with the order_status workload unless a lower-level function containing the LLM call is wrapped again and defines its own scoped workload_id.\n",
    "\n",
    "**Default Scoping Behavior**: By default, each trace inherits a workload_id from its parent NeMo Agent Toolkit registered function. The combination of client_id and workload_id is then used by the Data Flywheel to filter and select data for subsequent training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to use the example queries to interact with the application. Once a query completes, you can view the exported traces in the Elasticsearch index.\n",
    "\n",
    "The following command retrieves the most recent logged schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl -s \"http://localhost:9200/flywheel/_search?size=1&sort=timestamp:desc\" | jq '.hits.hits[0]._source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NAT implementation of AIVA also comes with [Phoenix observability plugin](https://docs.nvidia.com/nemo/agent-toolkit/latest/workflows/observe/observe-workflow-with-phoenix.html) to help you better visualize the agent trace/trajectory. To use Phoenix, navigate to your Brev instance page, go to the **Access** tab, select **Using Secure Links**, find and click the link that looks like `https://phx0-xxxxxxxx.brevlab.com` which will take you the Phoenix UI (shown below).\n",
    "\n",
    "![phoenix_ui](https://raw.githubusercontent.com/NVIDIA-AI-Blueprints/data-flywheel/fce0d3864d3587f1545fea8fb2f451ae02c082cd/notebooks/img/phoenix-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "## 3. Run a Flywheel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Sample Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue interacting with the AIVA application to generate logs for Flywheel, which helps drive meaningful improvements over the base candidate models. To speed up the process, we’ve also provided a sample set of pre-generated AIVA logs that you can load directly into Elasticsearch. \n",
    "\n",
    "This sample was created by first generating synthetic user queries with NeMo Data Designer, then passing those queries into AIVA to produce logs. NeMo Data Designer is purpose-built for AI developers to create high-quality, domain-specific synthetic data at scale—unlike general-purpose LLMs that often struggle to deliver consistent and reliable results. You can start from scratch or use your own seed datasets to accelerate AI development with greater accuracy and performance. To learn how to generate synthetic user queries using NeMo Data Designer, check out the [SDG notebook](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/blob/fce0d3864d3587f1545fea8fb2f451ae02c082cd/notebooks/data-flywheel-sdg.ipynb).\n",
    "\n",
    "First, we need to import required libraries and configure pandas display options for better readability in notebook outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Width of the display in characters\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided sample dataset from AI Virtual Assistant (`aiva`) (`data/aiva_primary_assistant_dataset.jsonl`) to simulate real user logs captured while an agentic customer service agent application is running. Each data point has the following schema:\n",
    "\n",
    "| Field        | Type               | Description                                                         |\n",
    "|--------------|--------------------|---------------------------------------------------------------------|\n",
    "| `timestamp`  | `int` (epoch secs) | Time the request was issued                                         |\n",
    "| `workload_id`| `str`              | Stable identifier for the logical task / route / agent node         |\n",
    "| `client_id`  | `str`              | Identifier of the application or deployment that generated traffic  |\n",
    "| `request`    | `dict`             | Exact [`openai.ChatCompletion.create`](https://platform.openai.com/docs/api-reference/chat/create) payload received by the model |\n",
    "| `response`   | `dict`             | Exact `ChatCompletion` response returned by the model               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `request` uses the OpenAI `ChatCompletions` request format and contains the following attributes:\n",
    "\n",
    "- `model` includes the Model ID used to generate the response.\n",
    "- `messages` includes a `system` message as well as a `user` query.\n",
    "- `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their parameters and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = data_dir / \"aiva_primary_assistant_dataset.jsonl\"\n",
    "\n",
    "!head -n1 {DATA_PATH} | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points generated by AI Virtual Assistant in response to user queries are considered **ground truth**. \n",
    "\n",
    "Ground truth data points are used to **evaluate** and **customize** more efficient models that can perform similarly to the current model. This customization process is analogous to a student-teacher distillation setup, where synthetic data generated from the teacher model is used to fine-tune a student model.\n",
    "\n",
    "Next, we'll load the data into Elasticsearch using a helper method `load_data_to_elasticsearch`, making it accessible to the Data Flywheel service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.load_test_data import load_data_to_elasticsearch\n",
    "\n",
    "load_data_to_elasticsearch(file_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Launch Flywheel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a Flywheel job by sending a POST request to the `/jobs` API. This triggers the workflow asynchronously.\n",
    "\n",
    "In production environments, you can automate this process to run at scheduled intervals, in response to specific events, or on demand.\n",
    "\n",
    "For this tutorial, we will target the primary customer service agent by setting the `workload_id` to \"primary_assistant\" and we will set `client_id` to \"aiva-1\" which has 300 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flywheel Service URL\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-1\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each candidate model, the data flywheel runs evaluations on the base model and its in-context learning (ICL) variant. If customization is enabled, the model is fine-tuned and evaluated again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "## 4. Monitor Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Check Job Status\n",
    "\n",
    "Submit a GET request to `/jobs/{job_id}` to retrieve the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_id):\n",
    "    \"\"\"Get the current status of a job.\"\"\"\n",
    "    response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the job status output, you will see the following metrics for evaluating the accuracy of tool calling once evaluations get completed:\n",
    "\n",
    "| Metric Name                                   | Definition                                                                                                         | Scoring Criteria                                                                                                         | Notes                                                                                                                        |\n",
    "|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Function name accuracy**                         | Checks if the predicted function name exactly matches the ground truth function name.                              | 1 if predicted function name is an exact match; 0 otherwise.                                                             | Evaluates only the function name, not arguments.                                                                             |\n",
    "| **Function name + args accuracy (exact-match)**  | Checks if both the function name and all arguments exactly match the ground truth.                                  | 1 if both function name and all arguments are exact matches; 0 otherwise.                                                | Strictest metric; all parts must match exactly.                                                                              |\n",
    "| **Function name + args accuracy (LLM-judge)**    | Checks if the function name matches exactly, and arguments are either exact matches or semantically equivalent.     | 1 if function name matches and each argument is either an exact match or semantically correct (as judged by an LLM); 0 otherwise. | Allows semantic similarity for complex arguments; captures intent and functional correctness even with paraphrasing.          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process and enable continuous monitoring, we defined a utility function `monitor_job` in `utils/job_monitor_helper.py`:\n",
    "\n",
    "- Periodically retrieve the job status\n",
    "- Format the output into a table\n",
    "- When any evaluations get completed, it fetches detailed results from the NeMo Evaluator Microservice, and uploads them to MLflow for visualization.\n",
    "\n",
    "This makes it easier to compare and analyze the results. \n",
    "\n",
    "### (Optional) Viewing the MLflow dashboard\n",
    "If MLflow visualization is enabled, the MLflow dashboard will be available at port 5000 (default)\n",
    "* **If using Brev Launchable:**\n",
    "    Navigate to your Brev instance page, go to the `Access` tab, select `Using Secure Links`, and click the link that looks like `https://mlflow-*.brevlab.com`. As evaluation jobs complete, they will be logged in MLflow with their flywheel `job_id` as the MLflow experiment name.\n",
    "* **If using a Self-Hosted Notebook Setup:**\n",
    "    Open your browser and go to `<local-IP>:5000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Run Continuous Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The first customization run typically takes about **10 minutes** to start while the training container is being downloaded. The `monitor_job` call in the cell below usually requires around **50 minutes** to complete training and evaluation of the candidate NIMs, though the exact duration may vary depending on the specific GPU and the responsiveness of the remote endpoint used for LLM-judge evaluations.  \n",
    ">  \n",
    "> As the evaluations complete, you will begin to see metrics appear in both the table and the MLflow dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils.job_monitor_helper import monitor_job\n",
    "\n",
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve now successfully completed a Flywheel run and can review the evaluation results to decide whether to promote the customized or ICL model. However, with only 300 data points, the customized `Llama-3.2-1B-instruct` is likely still limited in accuracy.\n",
    "\n",
    "That said, the Data Flywheel operates as a self-reinforcing cycle—models continue to improve as more user interaction logs are collected. Below, we demonstrate how the model accuracy improves incrementally with additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "## 5. Show Continuous Improvement (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the flywheel run with additional data, we’ll launch a new job using `client_id` set to \"aiva-2\", which includes **500** data points, to evaluate the impact of increased data volume on performance.\n",
    "\n",
    "Note that `client_id` is originally intended to identify the client that generated the traffic. However, in the notebook, it was repurposed to represent datasets of varying sizes, illustrating the progressive improvement of the data flywheel as more data is collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-2\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some improvements of the customized model compared to the last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have now collected even more data points, let's kick off another flywheel run by setting `client_id` to \"aiva-3\" which includes **1,000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-3\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_job(\n",
    "    api_base_url=API_BASE_URL,\n",
    "    job_id=job_id,\n",
    "    poll_interval=5\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run with 1,000 data points, we should observe the customized model’s accuracies improving significantly, with the Function name accuracy approaching 1.0. \n",
    "\n",
    "This indicates that the customized `LLama-3.2-1B-instruct` model achieves accuracy comparable to the much larger `LLama-3.3-70B-instruct` base model deployed in AI Virtual Assistant, while significantly reducing latency and compute usage thanks to its smaller size. \n",
    "\n",
    "In the next step, we will show how to deploy the customized `LLama-3.2-1B-instruct` and run inference with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"6\"></a>\n",
    "## 6. Deploy Customized Model for Latency Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a fine tuned smaller model that matches the accuracy of the larger model currently production, we can deploy the smaller model to replace the larger model in production. \n",
    "\n",
    "To do this, we will leverage two component microservices from the NeMo Microsevices Platform to simplify model deployment and inference:\n",
    "\n",
    "- NeMo Deployment Management: Provides an API to deploy NIM on a Kubernetes cluster and manage them through the NIM Operator microservice.\n",
    "- NeMo NIM Proxy: Provides a unified endpoint that you can use to access all deployed NIM for inference tasks.\n",
    "\n",
    "We will deploy the fine tuned `LLama-3.2-1B-instruct` model to replace the Primary Assistant LLM as an example. First, let's deploy the base model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Initialize Nemo Microservices Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_microservices import NeMoMicroservices\n",
    "\n",
    "# Configure microservice host URLs\n",
    "NEMO_BASE_URL = \"http://nemo.test\"\n",
    "NIM_BASE_URL = \"http://nim.test\"\n",
    "\n",
    "# Initialize the client\n",
    "nemo_client = NeMoMicroservices(\n",
    "    base_url=NEMO_BASE_URL,\n",
    "    inference_base_url=NIM_BASE_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deploy the Base Model using Nemo Deployment Management Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = nemo_client.deployment.model_deployments.create(\n",
    "    name=\"llama-3.2-1b-instruct\",\n",
    "    namespace=\"meta\",\n",
    "    config={\n",
    "        \"model\": \"meta/llama-3.2-1b-instruct\",\n",
    "        \"nim_deployment\": {\n",
    "            \"image_name\": \"nvcr.io/nim/meta/llama-3.2-1b-instruct\",\n",
    "            \"image_tag\": \"1.8.3\",\n",
    "            \"pvc_size\": \"25Gi\",\n",
    "            \"gpu\": 1,\n",
    "            \"additional_envs\": {\n",
    "                \"NIM_GUIDED_DECODING_BACKEND\": \"outlines\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the deployment status to become ready before proceeding to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    deployment_status = nemo_client.deployment.model_deployments.retrieve(\n",
    "        namespace=deployment.namespace,\n",
    "        deployment_name=deployment.name\n",
    "    )\n",
    "    status = deployment_status.status_details.status\n",
    "    if status == \"ready\":\n",
    "        break\n",
    "    time.sleep(5)\n",
    "\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the available models in NIM proxy. You should see the base model `meta/llama-3.2-1b-instruct` as well as fine tuned models that looks something like `dfwbp/adj@cust-EPqQfovXK2XPhF6tvVCQd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available NIMs for inference by their IDs and get the fine tuned model name for inference\n",
    "available_nims = nemo_client.inference.models.list()\n",
    "for nim in available_nims.data:\n",
    "    print(nim.id)\n",
    "    if nim.id.startswith(\"dfwbp\"):\n",
    "        ft_model_name = nim.id\n",
    "        os.environ[\"PRIMARY_ASSISTANT_LLM_MODELNAME\"] = ft_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the fine-tuned model, which can be obtained from the job status API:\n",
    "# response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")  # use the job_id for the best run based on eval results\n",
    "# ft_model_name = response.json()['nims'][0]['customizations'][0]['customized_model'] # change the first index to the model of your choice (default index is 0 which will use the first model listed in the config\n",
    "\n",
    "# get a example input request for inference\n",
    "with open(\"/home/ubuntu/data-flywheel/data/aiva_primary_assistant_dataset.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "random_line = random.choice(lines)\n",
    "input_data = json.loads(random_line)\n",
    "\n",
    "llm_output = nemo_client.chat.completions.create(\n",
    "    model=ft_model_name,\n",
    "    messages=input_data['request']['messages'],\n",
    "    tools=input_data['request']['tools'],\n",
    "    temperature=0.7,\n",
    "    max_tokens=8000,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(llm_output.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's swap out the `llama-3.3-70b-instruct` model with the fine tuned model for the primary assistant node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Deploy the Fine Tuned Model to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "cd ai-virtual-assistant\n",
    "\n",
    "export APP_CHAT_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export APP_CHAT_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export PRIMARY_ASSISTANT_LLM_SERVERURL=http://nim.test \n",
    "\n",
    "export ORDER_STATUS_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export ORDER_STATUS_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "export RETURN_PROCESSING_LLM_MODELNAME=meta/llama-3.3-70b-instruct\n",
    "# export RETURN_PROCESSING_LLM_SERVERURL=http://nim.test  ## uncomment if model deployed locally with NMP\n",
    "\n",
    "\n",
    "export APP_LLM_MODELENGINE=nvidia-ai-endpoints\n",
    "export DATA_FLYWHEEL_CLIENT_ID=nat-ai-virtual-assistant\n",
    "export DATA_FLYWHEEL_ENDPOINT=http://localhost:9200\n",
    "export DATA_FLYWHEEL_ES_INDEX=flywheel_test\n",
    "\n",
    "docker compose -f deploy/compose/docker-compose.nat.yaml down >> aiva_deploy.log 2>&1\n",
    "docker compose -f deploy/compose/docker-compose.nat.yaml up -d --build >> aiva_deploy.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After deploying the fine-tuned model, you go back to the AIVA UI to run some test queries with the fine-tuned model and evaluate its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
